# CrossEntropyLossFunction

Coding the binary Log Loss/Cross Entropy Error Function and break it down to the very basic details.

please make sure you know about Perceptron model firstðŸ›‘

Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.

Overall, as we can see the cross-entropy is simply a way to measure the probability of a model. The cross-entropy is useful as it can describe how likely a model is and the error function of each data point. It can also be used to describe a predicted outcome compare to the true outcome.
